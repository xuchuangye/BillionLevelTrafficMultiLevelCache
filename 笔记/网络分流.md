# 网络分流

到达服务之前，减少服务处理的请求数



## DNS

目的：将域名解析成IP地址

可以理解为：web服务，挂着一个分布式数据库，并且提供CRUD功能



域名和IP地址的映射类型：

-   A（Address）：域名和IP地址的对应关系
-   CName（Canonical Name）：域名和域名的对应关系
-   NS（Name Server）：域名和 能够解析该域名的域名服务器 的对应关系



## DNS服务器类型

### 根域名服务器

全球13个，美国10个，英国，瑞典，日本各1个

https://www.internic.net/domain/named.root

A：主根，B，C，D，E，F，G，H，I，J，L，M：辅根

**问题**：根域名服务器全球有13个，全球确实是有13台服务器吗？

不是



2020年8月份统计：1097个根域名服务器



根域名服务器的镜像服务器：

北京5个，上海1个，杭州2个，武汉1个，郑州1个，西宁，贵阳，广州各1个，香港9个，台北6个



## 域名解析流程

请求www.baidu.com

1.请求电脑本地local DNS，如果有，直接把域名对应的IP地址给客户端一个响应，如果没有

2.请求根域名服务器

3.请求顶级域名服务器

4.请求权威域名服务器

但是这种效率太低



缓存：

www.baidu.com

1.浏览器是否有域名的缓存

2.操作系统:hosts文件是否有域名的缓存

3.LDNS是否有域名的缓存

4.baidu.com NS域名服务器

减少实际请求中对根域名服务器的访问



问题：域名和域名的对应关系，怎么理解？

xuchuangye.com  <-> xrchiyou.com <-> 114.114.114.114

问题：开机的时候os会查询一次缓存下来把？

访问过一次之后也会

问题：GitHub访问慢是因为LDNS没有缓存吗？

不是，但是可以通过hosts文件提供访问速度

问题：我没有配置根域名服务器地址呀？

肯定接入电信、移动、联通的网络，运营商肯定知道根域名服务器的地址

不一定是本地去找，运营商ISP服务商也会去找

运营商也会存储这些域名的映射



**优化**：

**1.提前做好DNS缓存**

问题：用户怎么访问到我的缓存？

大公司建的DNS服务器，用户请求DNS服务器，公司很有实力，在北京放一个DNS服务器，让当前的运营商访问我的DNS服务器，提高解析我网站的访问速度

问题：直接访问IP地址不行吗？

1）记不住

2）IP地址会经常变

3）通过域名给IP地址做一层转换的话，灵活度更高，中间环节越多，灵活度越大，访问速度越低

**2.A（Address）记录，一个域名对应多个IP地址**，多记录IP地址

结论：通过DNS做负载均衡

优点：

-   配置简单，负载均衡不需要管理，DNS本来就支持，无成本费用

-   将负载均衡的工作交给DNS服务器，省去了管理的麻烦，DNS本来就支持

缺点：

-   时效性问题：记录的添加和修改是需要一定的时间才能生效，域名的有效性和服务的有效性不是实时的，可能用户会报错，或者访问已经挂掉的服务，或者新的服务用户访问不到

-   DNS的负载均衡算法无法控制，无法灵活使用，不能自定义高级算法



## CDN

CDN的全称是Content Delivery Network，即**内容分发网络**

结论：一个大型的系统不可能只有一个节点，肯定在全国各地都部署有节点

如何在请求多个节点时做优化？

1.将请求分配到多个节点上，减少每个节点的并发数

2.让用户请求落到离该用户最近（网络能够拓扑到的最近，前提是能够访问到）的地方，响应速度更快，用户体验更好





CDN适合存放静态资源，或者经常不变化的数据



### CDN的两个关键节点

源站：核心业务系统，所有信息的来源



缓存（边缘）节点：静态资源

用户请求服务，先去CDN节点，如果没有去源站去获取，根据配置规则决定从源站获取的数据是否在节点做缓存



问题：CDN缓存怎么清除？

CDN数据库，数据库中的数据怎么清除，可以通过新的 数据替换掉旧的 数据，或者设置过期时间



问题：CDN缓存需要代码实现吗？

一般不需要，做不了CDN，不代表不使用CDN







CDN工作流程：

![CDN工作流程图](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/CDN%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%9B%BE.png)





### CDN节点数：

优点：节点数越多，证明用户请求的中间环节越少，响应速度越快

缺点：成本极高，数据同步问题

结论：需要做一个平衡，知道取舍



总结：

CDN的优点：

-   减少每一个系统请求的并发数
-   减少了平均响应时间
-   减少网络的拥堵

CDN缓存的内容：

一般只缓存静态的内容，一些涉及到动态内容的请求仍然需要源站去处理

通过配置CDN的规则，来决定CDN缓存哪些信息



#### 美团原题：如何设计能够分布大型文件的CDN系统？



## CDN设计

**识别用户来源**：谁使用我这个系统，如何识别，根据IP地址查询用户地理信息

**如何做就近分发：**

1.www.mashibing.com CName www.mashibing.com 与 www.xuchuangye.com的对应关系



购买了两个CDN节点，一个北京的一个郑州的

```java
if(地理信息是否是北京的) {
  来自北京的用户指向北京
}else {
  来自郑州的用户指向郑州
}
```



一般的公司是没有实力做CDN，都是购买CDN节点以及CDN的服务

**做内容的缓存：**

/xxx.html，/xxx.png这些静态资源，是和缓存到CDN中

做好映射：key：/xxx.html，value：响应的值



**问题**：静态资源是推送到CDN做缓存，还是CDN提前拉取？

都可以



**扩展**：

数据来源：

1.启动APP或者网站时，初始化，数据准备好，内置好

初始化之后这些数据不变时，用户看到的肯定不是最新的，这时候就需要增量更新

2.增量更新，用户请求，查询源站，看一看。设置有效期，请求一个图片资源，如果五分钟之内这个资源不会变，所以直接给用户响应，超过五分钟再查看有没有新的资源



## CDN总结

CDN原理流程图

![CDN原理流程图](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/CDN%E5%8E%9F%E7%90%86%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

CDN服务商会维护用户的CDN节点（源站：提供动态内容）的列表



扩展（CDN工作原理类似于注册中心）：

分为两个步骤：

1.获取CDN节点的IP地址

2.获取内容的请求





## 多地址直连

**功能**：将用户的动态请求做分发



**角色**：

用户、调用方

CDN服务商（管理服务的通讯录）：维护用户的CDN节点（源站：提供动态内容）的列表

CDN节点：真正提供服务的节点



### **方案：**

### **方案一：注册中心的工作流程**

![注册中心的工作流程](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)



缺点：所有的请求都必须访问注册中心，注册中心负责全量的查询请求，压力非常大

优化：客户端缓存



更简化的方法：去掉注册中心，将选择权交给用户







如果服务的数量是明确的，可以进行简化

### **方案二：规则中心的工作流程**

![规则中心的工作流程](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/%E8%A7%84%E5%88%99%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

应用场景：

APP发布的时候，内置了一套解析规则的程序，按照规则：根据用户的地理位置和用户的id，分发到指定的服务节点，减少网络中对其他节点的请求，减少服务端的压力



## 代理

用户请求的确定的IP地址，这个IP地址不一定是真实提供服务的IP地址，只是服务提供商对外暴露的统一IP



### 正向代理

一个代理代理多个用户

![正向代理](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86.png)



### 反向代理

所有的请求由服务端接收，由代理服务端分发到服务节点，所有的请求都由一个服务器接收，无法判断这个服务代理了多少的服务器

![反向代理](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86.png)

解决高并发的场景，反向代理是用的最多的

**识别用户的请求：**

应用层 http ftp

表示层

会话层

传输层 TCP

网络层

数据链路层

物理层



**4层反向代理**：根据用户的IP和端口

**7层反向代理**：（包括4层的用户信息）根据用户的协议（http，ftp），请求方式（GET，POST），请求头，正文参数，cookie，特殊的资源（图像）

在哪一层能获的用户的什么信息，就根据什么信息来做转发



**4层反向代理和7层反向代理的区别：**

4层掌握用户的信息更少，实现更简单，运行效率更高，越接近物理层效率越高

7层反向代理可以收集更多的信息，可以更多的解析方式，实现更复杂，运行效率更低，但是更加智能



Nginx的

-   upstream：给Nginx提供支付的服务，Nginx的后方服务节点
-   access_by_lua_file：会根据lua的规则进行转发





## 负载均衡算法

**轮询**：RR（Round Robin），适用于所有的服务器硬件配置都一样的情况/场景

代码实现：

全局变量i，记录值，一开始是0，

每次将用户请求分发给服务节点时，都会i + 1，然后对服务器的总数进行取模



**权重**：WRR（Weight Round Robin），按照权重的不同进行分发

代码实现：

比如两个服务，一个权重是6 ，另一个权重是4。

1-10之间去 随机数。如果取到  1-6，那么 找6权重的服务。

如果取到 7-10，找 4权重的服务。



**随机**：Random



**哈希**：Hash，原地址散列，Source Hashing

111.111.111.111请求的IP，将该IP地址进行哈希，将结果做成对应到1号服务节点，输入和输出是对应关系

每次请求都能通过哈希算法，分配到固定的服务节点上

只要原地址不变，每次请求映射来，后面提供服务的节点也不会变

优点：便于session维护

每次请求，信息总是固定的，将请求路由到固定的服务节点上，并且将信息存储到该服务节点上，每次都去该服务节点上取，session非常好维护



**最少链接**：Least Connections，每个服务器正在处理的链接数

最符合负载均衡的算法，将请求路由到最少连接的服务器上

代码实现：

需要Redis中间存储每个服务节点上的连接数，那么Redis的存储结构：哈希，hset key field（服务器节点的标识）value（连接次数）





# 服务集群方案



## 并发和并行

到达服务之后，提升服务处理的请求数



并行：在同一时间点，有多个任务同时进行

只有在多核的情况下，并行才会发生

并发：在某个<u>时间点</u>，只有一个任务进行。但是在一个<u>时间段</u>，有多个任务同时进行

在多个任务中，每一个任务拆分成细小的任务片，从属于不同任务的任务片被轮着来处理

在整体的架构设计上，宏观上并行和并发都称之为并发



并发的处理方式或者如何让多个系统同时处理多个请求



## 集群（服务与服务之间的并发）



### 集群的缺点：

1.注意幂等（幂等：每次操作都是一样的结果）

在同一时间点，用户先后发出两个请求：A请求修改数据，B请求查询数据，用户预期的结果是B请求能够查询到A请求之后修改的数据，但是如果A和B请求分发到不同的服务节点，然后B请求先执行，A请求再执行，那么就会造成数据不一致的问题

![image-20230304083530629](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/image-20230304083530629-16780213723041.png)

所以重试的时候需要做幂等



2.注意数据存储的共享

服务节点之间需要做共享





问题：做一个数据共享的模块



问题：分布式事务



## 无状态节点集群



在同一时间点，用户先后发出两个请求：A请求修改数据，B请求查询数据，无论A和B的请求是否分发到同一个服务节点上，都不影响B请求结果的准确性

![image-20230304085012726](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/image-20230304085012726-16780213753982.png)





做集群的时候，不要修改服务内存的数据，而是进行数据存储的共享

**无状态**：没有特殊状态的服务，每一个请求对服务器来说都是统一无差别处理

请求到达服务器之后，携带了服务端所需要的参数，服务端（服务的内存）不存储所有跟请求相关的任何数据



**有状态**：在服务端存储之前的请求信息，用于后面的请求处理

举例：

用户登录服务器，服务器存储用户的session，用户下次请求传递session的id，服务器根据id查询session，从session中取出用户信息，知道用户的身份来进行登录，这就是有状态



有状态不便于集群的水平扩张，因为服务节点之间存储不同请求信息



集群一般是无状态的

确保无状态，就必须保证所有接口都是恒等类，即接口被调用前后，系统存储的数据不能发生变化，可以通过公共存储实现无状态



**无状态节点集群协作的问题：**

定时任务：发消息



内部加锁：在程序执行的时候，判断是否可以发

服务节点可以去一个公共的地方读，是否有服务已经发过消息了，如果没有就进行发消息，如果有则直接跳过



外部唤醒：外部唤醒只请求一次到服务端，然后只分发到一个服务节点

![image-20230304091132447](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/image-20230304091132447-16780213780463.png)



## 单一服务节点集群



一般选择服务器的游戏，是实时对战的游戏，一般是用长连接做的，不方便用户中途切换，而且将数据存储到不同的服务节点上

游戏一般会在缓存和数据库都存储



用户每次都访问到同一个服务节点，所以叫单一服务节点集群

关键：实现用户和服务器对应关系的映射，将固定的用户和固定的服务节点做关联

1.用户手动选择

2.用户id分配服务器



单一服务节点集群解决了有状态的问题

缺点：服务节点之间的数据是互相隔离的，一旦某一个服务节点崩溃了，就容易损失一部分用户的服务，容错性比较差



## 信息共享节点集群

所有的服务节点都连接在一个信息池中，并在信息池中存储用户的信息

多个服务共同连接：共享的存储

![image-20230304093651302](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/image-20230304093651302-16780213806354.png)



通过启动和部署多个服务节点的方式，来完成集群的扩张

**信息共享节点集群协作的问题：**

通过数据库自带的锁就能进行限制，降低代码实现复杂度

通过公共存储的互斥性就可以做到



优点：

扩展比较方便，启动一个服务

缺点：

程序的性能会受到共享存储的限制：存储容量，读写性能，故障的单点，程序的瓶颈





## 信息一致节点集群



读写分离，采用分流的思想

![image-20230304094807675](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/image-20230304094807675-16780213828675.png)



存储之间：数据一致性的问题，以及延时的问题

用户在某个服务节点上进行写的操作，经过一段时间之后，能够从其他服务节点上读取到写操作之后的数据



**适用的场景**：

读多写少



强一致性：必须一致

最终一致性：可以忍受中间不一致，但是最终还是一致的



## 分布式系统

在集群当中，将一个服务节点的并发请求分发到多个服务节点上，降低了每个服务节点的压力

在之前的集群中，各个节点是同斥的，大家都一样，各自运行一套完整且相同的代码，如果应用比较复杂，性能会受到共享存储的限制

业务逻辑变复杂，变更维护变复杂

目的：统一接口的定义，彼此当成黑盒



微服务是分布式系统的解决方案之一



总结：

最开始只有<u>一个服务</u>，如何让服务承接更多的请求

可以有多个服务，也就是服务的分身（<u>复制多份</u>）

随着功能越来越复杂，项目越来越庞大，代码越来越多，发现有些功能需要的硬件配置是不一样的

那么就需要服务的<u>垂直切分</u>



**水平拆分和垂直切分的区别：**

水平拆分是拆结构

![image-20230304100903785](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/image-20230304100903785-16780213852836.png)

垂直切分是切业务

![image-20230304101031453](%E7%BD%91%E7%BB%9C%E5%88%86%E6%B5%81.assets/image-20230304101031453-16780213871257.png)



## 如何实现高并发

亿级流量多级缓存的应用场景：在某一个时间点对热点数据的大规模访问

高并发系统中的代码就是普通的代码

通过大量的机器配置、架构设计实现高并发的流量

项目突然涌入大量的流量：

1.购买机器，先采取应急方案

2.优化架构，巨石系统会随着用户变多，需求变多，功能变复杂，需要进行拆分，一个项目拆分成多个项目就需要架构的设计

3.沉淀技术（中间件），公司针对特殊的请求会沉淀技术，一些请求走缓存就沉淀出缓存中间件，请求走消息队列就异步削峰解耦，沉淀出消息中间件，大公司靠中间件承接大量的流量，业务代码还是CRUD

4.如果流量还在继续增加，单纯的靠人工运维是很困难的，这时候就需要自动化扩容，推荐阿里云（经过双十一的检验）

虽然加机器能够解决很多问题，为什么还要使用架构的设计，让我们的业务更便于扩张，更省机器



## CAP

C：一致性，A：可用性，P：分区容错性，不可能同时满足，P是必选项，从A和C中二选一

### 程序员越往上发展，设计和理论知识越显得非常重要，是理解分布式系统的起点



数据的一致性问题，从什么角度去分析？

读，写

数据不一致是因为有人去写（修改）：该数据了，发现数据不一致是因为有人去读（查询）：该数据了



P：分区容错性

分区：**一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。**

容错：**当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。**

**提高分区容错性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。**

**然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。**

**总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。**



分布式系统有多个节点，如果每个节点网络发生故障，如果不保证分区容错性，这个系统就崩溃了，违背了分布式系统存在的意义



问题：形成分区的原因只有网络故障这一种吗？



### 网络八大谬误：

1.   网络总是可靠的，网络的硬件和软件出错，网络管理员修改路由器的配置时出错，网络中的各个节点停电了
2.   网络没有延迟，等待响应所花费的时间，只要存在距离和速度就必然会有网络延迟
3.   网络的带宽无限，带宽的上行和下行都有限制
4.   网络总是安全的，病毒，网络攻击，黑客
5.   网络拓扑不会改变，网络拓扑图：网络节点设备和通信介质构成的网络结构，多个设备之间的线连接的结构就叫网络拓扑图，可以理解为部署好的节点以及节点之间的调用链路
6.   网络只有一个管理员，特别是云服务，一定会有多个管理员
7.   网络传输代价为0，网络传输会耗电，
8.   网络是同构的，网络会连接计算机和其他设备，每个设备会有不同的操作系统、传输协议

P：必须保证，即使是网络出现问题，这个系统也要能够正常运行，否则分布式系统就没有存在的意义



问题：如何保证P，方式有哪些？

数据复制到多个节点上

举例：

MySQL的主从，Redis的主从，zookeeper的leader和flower



### 为什么只能选择AP或者CP



一致性：写什么，就能读什么，写操作：原子性的操作

强一致性：写操作之后，后续的所有读操作都能够读到新值

弱一致性：写操作之后，读该数据时，可能是新值，也可能是旧值

最终一致性：写操作之后，在中间的一段时间内可能读的是旧值，但最终读的是新值



可用性：向为崩溃的服务节点发请求，总是能够收到响应，有数据就行，不管数据的一致性



一致性和可用性如何选择

根据企业需求，或者说容忍度

（一般从中间件的角度去分析：MySQL，Redis，zookeeper）



### 大部分公司采用不错的策略：

先保证可用性和分区容错性，然后再兼顾一致性（舍弃强一致性，保证最终一致性）

举例：

电商，买东西，送积分。用户先购买商品，积分晚点发

微信抢红包



如果保证强一致性，会对吞吐量造成负面影响。

因为强一致性，会造成系统一段时间不可用，系统不可用，系统的吞吐量会降低





## 服务（服务内部的并发）

多进程、多线程、多协程



### 多进程

每个进程 之间，资源独立，具有很强的隔离性

Java -jar xxx.jar：启动一个Java进程



多进程的应用场景：

有限的物理机上做高可用

两台物理机：A，B，C三个服务，A在第一台部署一些数据，在第二胎部署一些数据，也就是混部



### 多线程

场景一：

一个方法，除了计算数值之后，还需要存储到本地磁盘（IO）

场景二：

出租车计费

时长计费（1min 多少钱），里程计费（1公里 多少钱）

可以同时进行计算，没有必要等计算完时长再计算里程



目的：

1.   提高效率，CPU使用率，节省时间，提高响应速度

2.   实现异步，提前释放主线程。降低平均响应时间，节省保持客户端和服务端连接的资源

     举例：

     记录日志，或者与第三方交互，发消息

### 线程数的计算

实际工作中线程数是一件头疼的事情



公式一：

```
线程数 = CPU核数 * CPU利用率 * (1 + w / c)
```

**CPU利用率**：CPU工作率，0 ~ 1之间，在1s之内，CPU工作了0.5s，那么CPU的利用率就是50%

双核：0.4，0.6，CPU平均利用率50%

实际工作中，CPU利用率一般按照100%来进行计算

**w（wait：等待时间）**：

**c（computer：计算时间）**：



举例：

2核的CPU，等待时间2ms，计算时间1ms，那么线程数 = 2 * (1 + 2/1) = 6

结论：等待时间越长，线程数越多



公式二：

```
线程数 = CPU核数 / (1 - 阻塞系数)
```

阻塞系数（越阻塞数值越大）：计算密集型：0，IO密集型：1

阻塞的任务刚启动就卡死了，还没有到CPU就崩溃了，这就是1，这种任务是不存在的，无限趋近于1，但是不会到达1，到达1整个系统就挂掉了



统一公式：

```
CPU核数 * CPU利用率 * (1 + w / c) = CPU核数 / (1 - 阻塞系数)
```

阻塞系数 = w / w + c



问题：这两个公式计算的和压测的差距大不大？

实际以压测为准



如何决定QPS，TPS：

-   系统的配置
-   业务逻辑
-   网络情况
-   系统运行的情况



线程数、QPS、机器配置：

**全部以压测为准**





问题：IO密集型和计算密集型的计算方式一样吗？

IO密集型：w/c肯定大于1

计算密集型：w/c肯定小于1



系统配置，内存，需要上服务器测试，具体多少线程才合适，需要关注的指标：

-   CPU的使用率
-   线程数
-   耗时
-   内存消耗

